{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DoRA: Weight-Decomposed Low-Rank Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to minimize the number of trainable parameters efficiently. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Total Parameters: 4386178\n",
      "Base Model Trainable Parameters: 4386178\n",
      "\n",
      "LoRA Model Total Parameters: 4390274\n",
      "LoRA Model Trainable Parameters: 4096\n",
      "\n",
      "DoRA Model Total Parameters: 4390786\n",
      "DoRA Model Trainable Parameters: 4608\n",
      "\n",
      "Difference in Trainable Parameters (DoRA - LoRA): 512\n",
      "Percentage of Trainable Parameters in LoRA: 0.0934%\n",
      "Percentage of Trainable Parameters in DoRA: 0.1051%\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import logging\n",
    "import warnings\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Function to count total and trainable parameters\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "# Load a small pretrained model that can run on a local MacBook\n",
    "model_name = 'prajjwal1/bert-tiny'  # A tiny BERT model suitable for demonstration\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Count parameters in the base model\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print(f\"Base Model Total Parameters: {total_params}\")\n",
    "print(f\"Base Model Trainable Parameters: {trainable_params}\\n\")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "lora_config = LoraConfig(\n",
    "    r=4,  # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Count parameters after applying LoRA\n",
    "lora_total_params, lora_trainable_params = count_parameters(lora_model)\n",
    "print(f\"LoRA Model Total Parameters: {lora_total_params}\")\n",
    "print(f\"LoRA Model Trainable Parameters: {lora_trainable_params}\\n\")\n",
    "\n",
    "# Reset the model to the base state\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Apply DoRA to the model\n",
    "dora_config = LoraConfig(\n",
    "    r=4,  # Same rank as LoRA\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    use_dora=True,  # Enable DoRA\n",
    ")\n",
    "\n",
    "dora_model = get_peft_model(model, dora_config)\n",
    "\n",
    "# Count parameters after applying DoRA\n",
    "dora_total_params, dora_trainable_params = count_parameters(dora_model)\n",
    "print(f\"DoRA Model Total Parameters: {dora_total_params}\")\n",
    "print(f\"DoRA Model Trainable Parameters: {dora_trainable_params}\\n\")\n",
    "\n",
    "# Compare the number of trainable parameters\n",
    "print(f\"Difference in Trainable Parameters (DoRA - LoRA): {dora_trainable_params - lora_trainable_params}\")\n",
    "print(f\"Percentage of Trainable Parameters in LoRA: {100 * lora_trainable_params / total_params:.4f}%\")\n",
    "print(f\"Percentage of Trainable Parameters in DoRA: {100 * dora_trainable_params / total_params:.4f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
